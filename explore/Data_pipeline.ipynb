{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA PIPELINE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Data format</b><br>\n",
    "1 Row = 1 Example = 1 Cycle<br>\n",
    "Each cycle so far has:\n",
    " - Qdlin (1000,1)\n",
    " - Tdlin (1000,1)\n",
    " - Cdlin (1000,1) (WIP)\n",
    " - discharge_time (1,) (WIP)\n",
    " - IR (1,)\n",
    " - remaining_cycle_life (1,) <- target\n",
    " \n",
    "For every cell we create one TFRecord file where each row represents data for one cycle. Before model training we read data from all files, create one dataset and feed it to the model. <br>Below are two different ways of structuring this data while reading and writing. Depending on what input the final model needs, we can chose the appropriate dataset design."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.train import FloatList, Int64List, Feature, FeatureList, FeatureLists, SequenceExample, Features, Example\n",
    "from tensorflow.feature_column import numeric_column, make_parse_example_spec\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Conv1D, Flatten, TimeDistributed, Activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load only batch1 for testing\n",
    "path1 = Path(\"Data/batch1.pkl\")\n",
    "batch1 = pickle.load(open(path1, 'rb'))\n",
    "\n",
    "# remove batteries that do not reach 80% capacity\n",
    "del batch1['b1c8']\n",
    "del batch1['b1c10']\n",
    "del batch1['b1c12']\n",
    "del batch1['b1c13']\n",
    "del batch1['b1c22']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) With tf.train.Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "see Hands-On Machine Learning pp.416\n",
    "\n",
    "1. The get_cycle_features function fetches all features and targets from \n",
    "the batch1 file and convert to \"Example\" objects. Every Example contains \n",
    "data from one charging cycle.\n",
    "\n",
    "2. Create a \"Data/tfrecords\" directory.\n",
    "\n",
    "3. For each cell create a tfrecord file with the naming convention \"b1c0.tfrecord\".\n",
    "The SerializeToString method creates binary data out of the Example objects that can\n",
    "be read natively in TensorFlow.\n",
    "\"\"\"\n",
    "\n",
    "def get_cycle_example(cell, idx):\n",
    "    cycle_example = Example(\n",
    "        features=Features(\n",
    "            feature={\n",
    "                \"IR\": Feature(float_list=FloatList(value=[batch1[cell][\"summary\"][\"IR\"][idx]])),\n",
    "                \"Qdlin\": Feature(float_list=FloatList(value=batch1[cell][\"cycles\"][str(idx)][\"Qdlin\"])),\n",
    "                \"Tdlin\": Feature(float_list=FloatList(value=batch1[cell][\"cycles\"][str(idx)][\"Tdlin\"])),\n",
    "                \"Remaining_cycles\": Feature(int64_list=Int64List(value=[int(batch1[cell][\"cycle_life\"]-idx)]))\n",
    "            }\n",
    "        )\n",
    "    )\n",
    "    return cycle_example\n",
    "\n",
    "data_dir = \"Data/tfrecords/\"\n",
    "if not os.path.exists(data_dir):\n",
    "    os.mkdir(data_dir)\n",
    "    \n",
    "for cell in batch1:\n",
    "    filename = os.path.join(data_dir + cell + \".tfrecord\")\n",
    "    with tf.io.TFRecordWriter(filename) as f:\n",
    "        num_cycles = int(batch1[cell][\"cycle_life\"])-1\n",
    "        for cycle in range(num_cycles):\n",
    "            cycle_to_write = get_cycle_example(cell, cycle)\n",
    "            f.write(cycle_to_write.SerializeToString())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define columns for our dataset\n",
    "ir = numeric_column(\"IR\", shape=[])\n",
    "qdlin = numeric_column(\"Qdlin\", shape=[1000, 1])\n",
    "tdlin = numeric_column(\"Tdlin\", shape=[1000, 1])\n",
    "remaining_cycles = numeric_column(\"Remaining_cycles\", shape=[], dtype=tf.int64)\n",
    "\n",
    "columns = [ir, qdlin, tdlin, remaining_cycles]\n",
    "example_spec = make_parse_example_spec(columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Reading the remaining code from bottom to top:\n",
    "\n",
    "When writing to TFrecord we created one file for each cell. Now we merge the\n",
    "data back into one dataset and prepare it to be fed directly into a model.\n",
    "\n",
    "The interleave() method will create a dataset that pulls 4 file paths from the\n",
    "filepath_dataset and for each one calls the function \"read_tfrecords\". It will then\n",
    "cycle through these 4 datasets, reading one line at a time from each until all datasets\n",
    "are out of items. Then it gets the next 4 file paths from the filepath_dataset and\n",
    "interleaves them the same way, and so on until it runs out of file paths. \n",
    "Note: Even with parallel calls specified, data within batches is still sequential.\n",
    "\n",
    "The read_tfrecords() function reads a file, skipping the first row which in our case\n",
    "is 0/NaN most of the time. It then loops over each example/row in the dataset and\n",
    "calls the parse_feature function. Then it batches the dataset, so it always feeds\n",
    "multiple examples at the same time, and then shuffles the batches. It is important \n",
    "that we batch before shuffling, so the examples within the batches stay in order.\n",
    "\n",
    "The parse_features function takes an example and converts it from binary/message format\n",
    "into a more readable format. The make_parse_example_spec generates a feature mapping \n",
    "according to the columns we defined. To be able to feed the dataset directly into a\n",
    "Tensorflow model later on, we need to split the data into examples and targets (i.e. X and y).\n",
    "\"\"\"\n",
    "# define variables\n",
    "batch1_keys = ['b1c0', 'b1c1', 'b1c2', 'b1c3', 'b1c4', 'b1c5', 'b1c6', 'b1c7', 'b1c9', 'b1c11', 'b1c14', 'b1c15', 'b1c16', 'b1c17', 'b1c18', 'b1c19', 'b1c20', 'b1c21', 'b1c23', 'b1c24', 'b1c25', 'b1c26', 'b1c27', 'b1c28', 'b1c29', 'b1c30', 'b1c31', 'b1c32', 'b1c33', 'b1c34', 'b1c35', 'b1c36', 'b1c37', 'b1c38', 'b1c39', 'b1c40', 'b1c41', 'b1c42', 'b1c43', 'b1c44', 'b1c45']\n",
    "window_size = 5\n",
    "shift = 1\n",
    "stride = 1\n",
    "\n",
    "def parse_features(example_proto):\n",
    "    examples = tf.io.parse_single_example(example_proto, example_spec)\n",
    "    targets = examples.pop(\"Remaining_cycles\")\n",
    "    return examples, targets\n",
    "\n",
    "def flatten_windows(features, target):\n",
    "    feat1 = features[\"Qdlin\"].batch(window_size)    \n",
    "    target_flat = target.skip(window_size-1)\n",
    "    return tf.data.Dataset.zip((feat1, target_flat))\n",
    "\n",
    "def flatten_windows_all_features(features, target):\n",
    "    \"\"\"\n",
    "    This method returns all features instead of just one, but \n",
    "    messes up the input shape. This method is not used at the\n",
    "    moment, but after we  figure out how to feed every feature\n",
    "    to the model, it could replace 'flatten_windows()'.\n",
    "    \"\"\"\n",
    "    feat1 = features[\"IR\"].batch(window_size)\n",
    "    feat2 = features[\"Qdlin\"].batch(window_size)\n",
    "    feat3 = features[\"Tdlin\"].batch(window_size)\n",
    "    features = tf.data.Dataset.zip((feat1, feat2, feat3))\n",
    "    target_flat = target.skip(window_size-1)\n",
    "    return tf.data.Dataset.zip((features, target_flat))\n",
    "\n",
    "def read_tfrecords(file):\n",
    "    dataset = tf.data.TFRecordDataset(file).skip(1) # skip can be removed when we have clean data\n",
    "    dataset = dataset.map(parse_features)\n",
    "    dataset = dataset.window(size=window_size, shift=shift, stride=stride, drop_remainder=True)\n",
    "    dataset = dataset.flat_map(flatten_windows)\n",
    "    dataset = dataset.shuffle(1000).batch(10).prefetch(1) # prefetch is only relevant for CPU to GPU pipelines, see Hands-On ML p.411\n",
    "    return dataset\n",
    "\n",
    "# define files to read from and store in a list_files object\n",
    "filepaths = [os.path.join(\"Data/tfrecords/\" + cell + \".tfrecord\") for cell in batch1_keys] \n",
    "filepath_dataset = tf.data.Dataset.list_files(filepaths)\n",
    "\n",
    "dataset = filepath_dataset.interleave(read_tfrecords, cycle_length=4, num_parallel_calls=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To feed it into a CNN with a Timedistibuted layer, we need this input:<br>\n",
    "Examples: [ batch_size, window_size, steps, input_dim ]<br>\n",
    "e.g.: [ 10, 5, 1000, 1 ]<br>\n",
    "Laels labels are contained in dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for feature, target in dataset.take(1):\n",
    "    print(\"Input shape: %s\" % [*feature.shape])\n",
    "    print(\"Target: %s\" % target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test with model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test dataset compatibility with a CNN + LSTM model layout\n",
    "steps = 1000\n",
    "input_dim = 1\n",
    "\n",
    "model = Sequential()\n",
    "# define CNN model\n",
    "model.add(TimeDistributed(Conv1D(filters=1, kernel_size=3, activation='relu'), input_shape=(window_size,steps,input_dim)))\n",
    "model.add(TimeDistributed(Flatten()))\n",
    "model.add(TimeDistributed(Dense(1)))\n",
    "# define LSTM model\n",
    "model.add(LSTM(50, activation='relu'))\n",
    "model.add(Dense(1))\n",
    "model.compile(loss=\"mean_squared_error\", optimizer='adam')\n",
    "model.fit(dataset, epochs=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) With tf.train.SequenceExample (and FeatureLists)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Same as in 1), but with an additional layer that sorts features into two categories \n",
    " - \"context\": \"IR\", \"Remaining_cycles\", \"Discharge_time\" (WIP) \n",
    " - \"details\": \"Qdlin\", \"Tdlin\"\n",
    "This stores Qdlin and Tdlin in one matrix which we use access more easily as input,\n",
    "but we lose their name handle. \n",
    "\"\"\"\n",
    "\n",
    "def get_sequence_example(cell, idx):\n",
    "    ir = Feature(float_list=FloatList(value=[batch1[cell][\"summary\"][\"IR\"][idx]]))\n",
    "    qdlin = Feature(float_list=FloatList(value=batch1[cell][\"cycles\"][str(idx)][\"Qdlin\"]))\n",
    "    tdlin = Feature(float_list=FloatList(value=batch1[cell][\"cycles\"][str(idx)][\"Tdlin\"]))\n",
    "    remaining_cycles = Feature(float_list=FloatList(value=[int(batch1[cell][\"cycle_life\"]-idx)]))\n",
    "\n",
    "    detail_features = FeatureList(feature=[qdlin, tdlin])\n",
    "\n",
    "    cycle_example = SequenceExample(\n",
    "        context = Features(\n",
    "            feature={\"IR\":ir,\n",
    "                     \"Remaining_cycles\": remaining_cycles\n",
    "                    }\n",
    "        ),\n",
    "        feature_lists = FeatureLists(feature_list={\"Details\":detail_features})\n",
    "    )\n",
    "    return cycle_example\n",
    "    \n",
    "data_dir = \"Data/tfrecords_featurelists/\"\n",
    "if not os.path.exists(data_dir):\n",
    "    os.mkdir(data_dir)\n",
    "    \n",
    "for cell in batch1:\n",
    "    filename = os.path.join(data_dir + cell + \".tfrecord\")\n",
    "    with tf.io.TFRecordWriter(filename) as f:\n",
    "            num_cycles = int(batch1[\"b1c0\"][\"cycle_life\"])-1\n",
    "            for cycle in range(num_cycles):\n",
    "                cycle_to_write = get_sequence_example(cell, cycle)\n",
    "                f.write(cycle_to_write.SerializeToString())\n",
    "    break # write only one cell for testing. Remove this to write all cells from batch1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define variables\n",
    "batch1_keys = ['b1c0', 'b1c1', 'b1c2', 'b1c3', 'b1c4', 'b1c5', 'b1c6', 'b1c7', 'b1c9', 'b1c11', 'b1c14', 'b1c15', 'b1c16', 'b1c17', 'b1c18', 'b1c19', 'b1c20', 'b1c21', 'b1c23', 'b1c24', 'b1c25', 'b1c26', 'b1c27', 'b1c28', 'b1c29', 'b1c30', 'b1c31', 'b1c32', 'b1c33', 'b1c34', 'b1c35', 'b1c36', 'b1c37', 'b1c38', 'b1c39', 'b1c40', 'b1c41', 'b1c42', 'b1c43', 'b1c44', 'b1c45']\n",
    "window_size = 5\n",
    "shift = 1\n",
    "stride = 1\n",
    "\n",
    "context_feature_description = {\n",
    "    \"IR\": tf.io.FixedLenFeature([], tf.float32),\n",
    "    \"Remaining_cycles\": tf.io.FixedLenFeature([], tf.float32)\n",
    "}\n",
    "\n",
    "sequence_feature_description = {\n",
    "    \"Details\": tf.io.FixedLenSequenceFeature([1000], tf.float32),\n",
    "}\n",
    "    \n",
    "def parse_features(example_proto):\n",
    "    sequence_example = tf.io.parse_single_sequence_example(\n",
    "        example_proto,\n",
    "        context_feature_description,\n",
    "        sequence_feature_description\n",
    "    )\n",
    "    targets = sequence_example[0].pop(\"Remaining_cycles\")\n",
    "    return sequence_example, targets\n",
    "\n",
    "def flatten_windows(features, target):\n",
    "    feat1 = features[1][\"Details\"].batch(window_size)\n",
    "    feat2 = features[0][\"IR\"].batch(window_size)\n",
    "    target_flat = target.skip(window_size-1)\n",
    "    return tf.data.Dataset.zip((feat1, feat2, target_flat))\n",
    "\n",
    "def read_tfrecords(file):\n",
    "    dataset = tf.data.TFRecordDataset(file).skip(1) # skip can be removed when we have clean data\n",
    "    dataset = dataset.map(parse_features)\n",
    "    dataset = dataset.window(size=window_size, shift=shift, stride=stride, drop_remainder=True)\n",
    "    dataset = dataset.flat_map(flatten_windows).batch(8).shuffle(1000)\n",
    "    return dataset\n",
    "\n",
    "filepaths = [os.path.join(\"Data/tfrecords_featurelists/\" + cell + \".tfrecord\") for cell in batch1_keys] \n",
    "filepath_dataset = tf.data.Dataset.list_files(filepaths)\n",
    "\n",
    "dataset = filepath_dataset.interleave(read_tfrecords, cycle_length=4, num_parallel_calls=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Instead of the standard way to return data - (Features, Target) - \n",
    "this dataset returns (Details, Context, Target). This can be changed\n",
    "in flatten_windows().\n",
    "One way to retain the (Features, Target) structure would be to zip\n",
    "feat1 and feat2 before returning everything. But this would nest \n",
    "the data so it can't be accessed as easily.\n",
    "\"\"\"\n",
    "\n",
    "# The Qdlin/Tdlin matrix needs to be (1000,2) instead of (2,1000)\n",
    "for ex in dataset.take(1):\n",
    "    print(\"Qdlin and Tdlin:\\n %s\" % ex[0].shape)\n",
    "    print(\"IR:\\n %s\" % ex[1].shape)\n",
    "    print(\"Target: \\n%s\" % ex[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test with model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shapes don't match yet"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
