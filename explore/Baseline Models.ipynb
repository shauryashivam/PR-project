{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Models and Classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook adapts the feature engineering from the original paper to our windowed approach.  We use linear & other simple regression models here to serve as a baseline for the deep learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import ElasticNet, LinearRegression, LogisticRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/rebuild_windowed_features_20_5_1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing and feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_1_2_keys = df['cell_key'][df['cell_batch']!=3].unique()\n",
    "train_keys = batch_1_2_keys[1::2]\n",
    "test_keys = batch_1_2_keys[0::2]\n",
    "train_ind = df[df['cell_key'].isin(train_keys)].index\n",
    "test_ind = df[df['cell_key'].isin(test_keys)].index\n",
    "secondary_test_ind = df[df['cell_batch']==3].index\n",
    "\n",
    "splits = [train_ind, test_ind, secondary_test_ind]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define feature and target columns for regression models\n",
    "\n",
    "varmod_features = [\"variance_dQ_window\"]\n",
    "dismod_features = [\n",
    "    \"variance_dQ_window\",\n",
    "    \"minimum_dQ_window\",\n",
    "    \"skewness_dQ_window\",\n",
    "    \"kurtosis_dQ_window\",\n",
    "    \"discharge_capacity_1\",\n",
    "    \"diff_discharge_capacity_max_1\",\n",
    "]\n",
    "fullmod_features = [\n",
    "    \"minimum_dQ_window\",\n",
    "    \"variance_dQ_window\",\n",
    "    \"slope_lin_fit_window\",\n",
    "    \"intercept_lin_fit_window\",\n",
    "    \"discharge_capacity_1\",\n",
    "    \"mean_discharge_time\",\n",
    "    \"minimum_IR_window\",\n",
    "    \"diff_IR_window\",\n",
    "]\n",
    "targetmod = [\"target_remaining\"]  # , \"target_current\"\n",
    "\n",
    "# Define feature and target columns for classifiers\n",
    "\n",
    "varclf_features = [\"variance_dQ_window\"]\n",
    "fullclf_features = [\n",
    "    \"minimum_dQ_window\",\n",
    "    \"variance_dQ_window\",\n",
    "    \"discharge_capacity_1\",\n",
    "    \"diff_IR_window\",\n",
    "]\n",
    "targetclf = [\"target_classifier\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_split(data, features, target, split):\n",
    "    X = data.iloc[split,:].loc[:,features]\n",
    "    y = data.iloc[split,:].loc[:,target]\n",
    "    return X, y\n",
    "\n",
    "def eval_model(model, data, features, target, splits):\n",
    "    mse = list()\n",
    "    mae = list()\n",
    "    mpe = list()\n",
    "    for split in splits:\n",
    "        X, y = get_split(data, features, target, split)\n",
    "        pred = model.predict(X)\n",
    "        mse.append(mean_squared_error(pred, y))\n",
    "        mae.append(float(np.mean(np.abs(y-pred.reshape(-1,1)))))\n",
    "        mpe.append(float(np.mean(np.abs((y - pred.reshape(-1,1))) / y * 100)))\n",
    "    return mse, mae, mpe\n",
    "\n",
    "def eval_classifier(model, data, features, target, splits):\n",
    "    acc = list()    \n",
    "    for split in splits:\n",
    "        X, y = get_split(data, features, target, split)\n",
    "        pred = model.predict(X)\n",
    "        acc.append(accuracy_score(pred, y.values.ravel()))\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variance Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Elastic net\n",
    "x_train, y_train = get_split(df, varmod_features, targetmod, train_ind)\n",
    "\n",
    "alphas = np.linspace(0.0001,1,30)\n",
    "parameters = {\n",
    "    \"alpha\": alphas,\n",
    "    \"l1_ratio\": [0.01, 0.25, 0.5, 0.75, 1.]\n",
    "}\n",
    "enet = ElasticNet(random_state=54)\n",
    "regr = GridSearchCV(enet, parameters, cv=4, iid=False)\n",
    "print(\"Elastic Net: %s\" % regr.fit(x_train, y_train).score(x_train, y_train))\n",
    "\n",
    "\"\"\"\n",
    "Because an elastic net with alpha = 0 is technically a linear regression\n",
    "and elastic net produces inaccuracies with a small alpha,\n",
    "we also train a linear regression model.\n",
    "Linear regression performs slighty better at RMSE,\n",
    "Elastic net performs slightly better at MPE.\n",
    "We decide to take the linear regression scores.\n",
    "\"\"\"\n",
    "lin_reg = LinearRegression()\n",
    "print(\"Linear Regression: %s\" % lin_reg.fit(x_train, y_train).score(x_train, y_train))\n",
    "\n",
    "varmod_mse, varmod_mae, varmod_mpe = eval_model(lin_reg, df, varmod_features, targetmod, splits)\n",
    "\n",
    "\n",
    "# Add Random Forest\n",
    "rf_params = {\n",
    "    \"max_depth\": [2, 3],\n",
    "    \"n_estimators\": [10, 100]\n",
    "}\n",
    "rfst = RandomForestRegressor(random_state=54)\n",
    "rfst_grid = GridSearchCV(rfst, rf_params, cv=4, iid=False)\n",
    "print(\"Random Forest: %s\" % rfst_grid.fit(x_train, y_train).score(x_train, y_train))\n",
    "\n",
    "varmod_rf_mse, varmod_rf_mae, varmod_rf_mpe = eval_model(rfst_grid, df, varmod_features, targetmod, splits)\n",
    "print('varmod_rf_mse', varmod_rf_mse)\n",
    "print('varmod_rf_mae', varmod_rf_mae)\n",
    "print('varmod_rf_mpe', varmod_mpe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discharge Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Elastic net\n",
    "x_train, y_train = get_split(df, dismod_features, targetmod, train_ind)\n",
    "\n",
    "alphas = np.linspace(0.1,1,20)\n",
    "parameters = {\n",
    "    \"alpha\": alphas,\n",
    "    \"l1_ratio\": [0.01, 0.25, 0.5, 0.75, 1.]\n",
    "}\n",
    "enet = ElasticNet(random_state=54)\n",
    "regr = GridSearchCV(enet, parameters, cv=4, iid=False)\n",
    "print(\"Elastic Net: %s\" % regr.fit(x_train, y_train).score(x_train, y_train))\n",
    "\n",
    "dismod_mse, dismod_mae, dismod_mpe = eval_model(regr, df, dismod_features, targetmod, splits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Train Elastic net model\n",
    "# raising the alpha minimum to 0.59 silences the convergence warnings,\n",
    "# but decreases the score significantly - what's wrong here? \n",
    "\n",
    "x_train, y_train = get_split(df, fullmod_features, targetmod, train_ind)\n",
    "\n",
    "alphas = np.linspace(0.001,1,20)\n",
    "parameters = {\n",
    "    \"alpha\": alphas,\n",
    "    \"l1_ratio\": [0.001, 0.75, 1.]\n",
    "}\n",
    "enet = ElasticNet(random_state=54)\n",
    "regr = GridSearchCV(enet, parameters, cv=4, iid=False)\n",
    "print(\"Elastic Net: %s\" % regr.fit(x_train, y_train).score(x_train, y_train))\n",
    "\n",
    "fullmod_mse, fullmod_mae, fullmod_mpe = eval_model(regr, df, fullmod_features, targetmod, splits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate all linear regression models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({\"Model\":[\"Variance model\", \"Discharge model\", \"Full model\"],\n",
    "              \"MAE - Train\": [varmod_mae[0],dismod_mae[0],fullmod_mae[0]],\n",
    "              \"MAE - Primary test\": [varmod_mae[1],dismod_mae[1],fullmod_mae[1]],\n",
    "              \"MAE - Secondary test\": [varmod_mae[2],dismod_mae[2],fullmod_mae[2]],\n",
    "              \"MSE - Train\": [varmod_mse[0],dismod_mse[0],fullmod_mse[0]],\n",
    "              \"MSE - Primary test\": [varmod_mse[1],dismod_mse[1],fullmod_mse[1]],\n",
    "              \"MSE - Secondary test\": [varmod_mse[2],dismod_mse[2],fullmod_mse[2]],\n",
    "              \"MPE - Train\": [varmod_mpe[0],dismod_mpe[0],fullmod_mpe[0]],\n",
    "              \"MPE - Primary test\": [varmod_mpe[1],dismod_mpe[1],fullmod_mpe[1]],\n",
    "              \"MPE - Secondary test\": [varmod_mpe[2],dismod_mpe[2],fullmod_mpe[2]]\n",
    "             })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variance Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Train Logistic Regression\n",
    "x_train, y_train = get_split(df, varclf_features, targetclf, train_ind)\n",
    "\n",
    "parameters = {\"C\": [0.01,0.1,0.5,0.75,1]}\n",
    "\n",
    "logreg = LogisticRegression(solver=\"liblinear\", random_state=54)\n",
    "clf = GridSearchCV(logreg, parameters, cv=4, iid=False)\n",
    "print(\"Logreg: %s\" % clf.fit(x_train, y_train.values.ravel()).score(x_train, y_train.values.ravel()))\n",
    "\n",
    "varclf_acc = eval_classifier(clf, df, varclf_features, targetclf, splits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Logistic Regression\n",
    "# Why is the full classifier worse than the variance classifier?\n",
    "x_train, y_train = get_split(df, fullclf_features, targetclf, train_ind)\n",
    "\n",
    "parameters = {\"C\": [0.01,0.1,0.5,0.75,1]}\n",
    "\n",
    "logreg = LogisticRegression(solver=\"liblinear\", random_state=54)\n",
    "clf = GridSearchCV(logreg, parameters, cv=4, iid=False)\n",
    "print(\"Logreg: %s\" % clf.fit(x_train, y_train.values.ravel()).score(x_train, y_train.values.ravel()))\n",
    "\n",
    "fullclf_acc = eval_classifier(clf, df, fullclf_features, targetclf, splits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate all classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.DataFrame({\"Classifier\":[\"Variance classifier\", \"Full classifier\"],\n",
    "              \"Acc - Train\": [varclf_acc[0],fullclf_acc[0]],\n",
    "              \"Acc - Primary test\": [varclf_acc[1],fullclf_acc[1]],\n",
    "              \"Acc - Secondary test\": [varclf_acc[2],fullclf_acc[2]]})                                  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
